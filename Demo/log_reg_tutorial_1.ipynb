{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of machine learning\n",
    "\n",
    "![](./images/ml_types.jpg)\n",
    "\n",
    "(Image from https://technovert.com/introduction-to-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to classification with machine learning\n",
    "\n",
    "In classification tasks we seek to classify a 'case' into one or more classes, given one or more input features. This may be extended to enquiring about probability of classification. Examples of classification include:\n",
    "\n",
    "* What diagnosis should this patient be given?\n",
    "* What is the probability that an emergency department will breach four-hour waiting in the next two hours?\n",
    "* What treatment should a patient be given?\n",
    "* What is the probability that a patient will be re-admitted?\n",
    "\n",
    "**Reflection:**\n",
    "1. Can you think of three occasions where people make classifications?\n",
    "1. Can you think of an example where it might be useful to provide automated classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and logistic regression\n",
    "\n",
    "With ordinary regression we are trying to predict a value given one or more input features.\n",
    "\n",
    "![](./images/regression.png)\n",
    "\n",
    "*(Flashcard images from machinelearningflashcards.com)*\n",
    "\n",
    "With logistic regression we are trying to the predict the probability, given one or more inputs, that an example belongs to a particular class (e.g. *pass* vs *fail* in an exam). Our training data has the actual class (which we turn into 0 or 1), but the model returns a probability of a new example being in either class 0 or 1. The logistic regression fit limits the range of output to between 0 and 1 (whereas a linear regression could predict outside of this range).\n",
    "\n",
    "![](./images/logistic_regression.png)\n",
    "\n",
    "\n",
    "$$P = \\dfrac{e^{a+bX}}{1+e^{a+bX}}$$\n",
    "\n",
    "![](./images/logistic_regression_equation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data scaling\n",
    "\n",
    "It is usual to scale input features in machine learning, so that all features are on a similar scale. Consider these two features (we will create artifical data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two sets of data with different means and standard deviations\n",
    "np.random.seed(123) # The random seed simply \"fixes\" the random number generation to make it reproducible\n",
    "x1 = np.random.normal(50,10,size=1000)\n",
    "x2 = np.random.normal(150,30,size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up single plot\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "# Add histogram of x1\n",
    "ax.hist(x1, bins=50, alpha=0.5, color='b', label='x1')\n",
    "# Add histogram of x2\n",
    "ax.hist(x2, bins=50, alpha=0.5, color='r', label='x2')\n",
    "# Add labels\n",
    "ax.set_xlabel('value')\n",
    "ax.set_ylabel('count')\n",
    "# Add legend\n",
    "ax.legend()\n",
    "# Finalise and show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Normalisation\n",
    "\n",
    "With MinMax normalisation we scale all values between 0 and 1.\n",
    "\n",
    "$$z = \\frac{x-min(x)}{max(x) - min(x)}$$\n",
    "\n",
    "A less common alternative is to scale between -1 and 1.\n",
    "\n",
    "$$z = -1 + 2\\frac{x-min(x)}{max(x) - min(x)}$$\n",
    "\n",
    "Here we will use 0-1 normalisation.  Here we do this manually, but for real\n",
    "data we can use a MinMaxScaler (as seen in the lecture) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_norm = (x1 - x1.min()) / (x1.max() - x1.min())\n",
    "x2_norm = (x2 - x2.min()) / (x2.max() - x2.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation\n",
    "\n",
    "With standardisation we scale data such that all features have a mean of 0 and standard deviation of 1. To do this we simply subtract by the mean and divide by the standard deviation.\n",
    "\n",
    "$$z = \\frac{x-\\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, here we do this manually, but for real data we'd typically use a StandardScaler, as seen in the lecture :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_std = (x1 - x1.mean()) / x1.std()\n",
    "x2_std = (x2 - x2.mean()) / x2.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up three subplots (12 x 5 inch plot)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12,5))\n",
    "\n",
    "# Plot original data in axs[0]\n",
    "axs[0].hist(x1, bins=50, alpha=0.5, color='b', label='x1')\n",
    "axs[0].hist(x2, bins=50, alpha=0.5, color='r', label='x2')\n",
    "axs[0].set_xlabel('value')\n",
    "axs[0].set_ylabel('count')\n",
    "axs[0].legend()\n",
    "axs[0].set_title('Original data')\n",
    "\n",
    "# Plot normalised data in axs[1]\n",
    "axs[1].hist(x1_norm, bins=50, alpha=0.5, color='b', label='x1 norm')\n",
    "axs[1].hist(x2_norm, bins=50, alpha=0.5, color='r', label='x2 norm')\n",
    "axs[1].set_xlabel('value')\n",
    "axs[1].set_ylabel('count')\n",
    "axs[1].set_title('MinMax Normalised data')\n",
    "\n",
    "# Plot standardised data in axs[2]\n",
    "axs[2].hist(x1_std, bins=50, alpha=0.5, color='b', label='x1 norm')\n",
    "axs[2].hist(x2_std, bins=50, alpha=0.5, color='r', label='x2 norm')\n",
    "axs[2].set_xlabel('value')\n",
    "axs[2].set_ylabel('count')\n",
    "axs[2].set_title('Standardised data')\n",
    "\n",
    "# Adjust padding between subplots and show figure\n",
    "fig.tight_layout(pad=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes on scaling\n",
    "\n",
    "Most commonly, with classification:\n",
    "* Decision trees and random forests require no scaling\n",
    "* Linear regression and support vector machines use standardised input data\n",
    "* Neural networks use MinMax normalised input data\n",
    "\n",
    "Later we will see that when we split data into training and test sets we will scale based on min/max or mean/std of the training set data only. We will also use the scaling methods provided by the SciKit-Learn library rather than using our own."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
